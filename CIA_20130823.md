[word2vec](https://code.google.com/p/word2vec/)
===
* general introduction
	* an open source toolkit published by Google, aiming to learn the meaning behind words via deep learning
* features
	* takes a text corpus as input and produces word vectors as output
	* bag-of-words
	* skip-gram
	* requires no supervised information before or during training
* use cases
	* word clustering
		* it understands that Paris and France are related the same way Berlin and Germany are (capital and country), and not the same way Madrid and Italy are.
		* just read lots of news articles -- with no human supervision
			* places similar countries next to each other
			* arranges their capital cities in parallel
		* specific examples
		```
		->staple hammer saw drill
		I think staple doesnt belong in this list!

		->math shopping reading science
		I think shopping doesnt belong in this list!

		->rain snow sleet sun
		I think sun doesnt belong in this list!

		->eight six seven five three owe nine
		I think owe doesnt belong in this list!

		->breakfast cereal dinner lunch
		I think cereal doesnt belong in this list!
		```
	* Pre-trained entity vectors with Freebase naming
	* knowledge representation and extraction
	* machine translation
	* question answering
	* conversational systems

Ayasida
词聚类
===
* 横向组合 (syntagmatical) --> Brown聚类
	* [ToDo] [syntagma (句段，结构段)](http://baike.baidu.com/view/4083120.htm)
	
* 纵向组合 (paradigmatical) --> word2vec
	* paradigm (词形变化表)
		* able -> ability -> enable -> unable
		* across -> cross -> crossroad
		* act -> actor -> actress -> active -> activity
		